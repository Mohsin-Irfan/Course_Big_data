<html>
<body>
<h1 style="color:blue;">Big Data Analysis</h1>
</body>
</html>
Course Outline (Course Duration: 2 Months)
<h2>1. Introduction to Big Data Analysis Course:</h2>
Begin your acquisition of Big Data knowledge with the most up-to-date definition of Big Data. Youâ€™ll
explore the impact of Big Data on everyday personal tasks and business transactions with Big Data use
cases. Learn how Big Data uses parallel processing, scaling, and data parallelism. Learn about commonly
used Big Data tools. Then, go beyond the hype and explore additional Big Data viewpoints.
<h2>2. Course Learning Objectives</h2>
<html>
<body>
<table style="width:100%">
  <tr>
    <th>CLO #</th>
    <th>CLO Statement</th>
    <th>Total Weeks</th>
  </tr>
  <tr>
    <td>1</td>
    <td>To familiarize with python basics</td>
    <td>3</td>
  </tr>
  <tr>
    <td>2</td>
    <td>To understand the Data Analysis</td>
    <td>3</td>
  </tr>
   <tr>
    <td>3</td>
    <td>To understand the Big Data Engineering</td>
     <td>5</td>
  </tr>
</table>

<body>
<h2>3. List of Software</h2>

<table style="width:100%">
  <tr>
    <th>Sr.no. #</th>
    <th>Description</th>
    <th>Payment Detail</th>
  </tr>
  <tr>
    <td>1</td>
    <td>Python setup</td>
    <td>Open Source</td>
  </tr>
  <tr>
    <td>2</td>
    <td>Java setup</td>
    <td>Open Source</td>
  </tr>
   <tr>
    <td>3</td>
    <td>Apache Spark</td>
    <td>Open Source</td>
  </tr>
  <tr>
    <td>4</td>
    <td>Hadoop</td>
    <td>Open Source</td>
  </tr>
  <tr>
    <td>5</td>
    <td>AWS</td>
    <td>Free tier account</td>
  </tr>
</table>
</body>
  </body>
</html>

<h2>4. Lecture Breakdown</h2>
<h3> 1. Python</h3>
<h4>Week 1: Introduction to Python , &nbsp &nbsp &nbsp Python Data types (part 1) , &nbsp &nbsp &nbsp Python Data types (part 2)</h4>
<h4>Week 2: Conditional Statements and Loops , &nbsp &nbsp &nbsp Functions and Array</h4>
<h4>Week 3: Classes/Objects, Math, JSON, &nbsp &nbsp &nbsp File Handling</h4>
<h3> 2. Data Analysis</h3>
<h4>Week 1: Data Analysis ,  &nbsp &nbsp &nbsp Introduction to Big Data and PySpark (part 1)</h4>
<h4>Week2: PySpark (part 2)</h4>
<h3> 3. DataBase</h3>
<h4>Week 1: DataBase , &nbsp &nbsp &nbsp MongoDB , &nbsp &nbsp &nbsp Integration</h4>
<h3> 4. Pipeline and ETL</h3>
<h4>Week1: Pipeline and ETL (part 1) , &nbsp &nbsp &nbsp Pipeline and ETL (part 2)</h4>
<h4>Week2: Pipeline and ETL (part 3)</h4>
<h3> 5. Hadoop and Hive SQL</h3>
<h4>Week1: Hadoop and Hive SQL (Part 1)</h4>
<h4>Week2: Hadoop and Hive SQL (Part 2)</h4>
--------------------------------------------------------------------------------------------------------------------------------------------------------------
<h2>5. Introduction to the Topics</h2>
<h5>5.1.Introduction to Python:</h5>
<h5>This is chapter One where we explore what it means to write programs. Where, you will set things up so
you can write Python programs. Moreover, in this first chapter we try to cover the "big picture" of
programming so you get a "table of contents" of the rest of the book.
</h5>
<h5>5.2.Python Data types (part 1):</h5>
<h5>In this lecture we will be covering Strings and Lists, and moving into data structures. As we want to solve
more complex problems in Python, we need more powerful variables. Starting with lists we will store
many values in a single variable using an indexing scheme to store, organize, and retrieve different values
from within a single variable.</h5>
<h5>5.3.Python Data types (part 2):</h5>
<h5>The Python dictionary is one of its most powerful data structures. Instead of representing values in a
linear list, dictionaries store data as key / value pairs. Using key / value pairs gives us a simple in-memory
"database" in a single Python variable. Tuples are our third and final basic Python data structure. Tuples
are a simple version of lists. We often use tuples in conjunction with dictionaries to accomplish multi-step
tasks like sorting or looping through all of the data in a dictionary</h5>
<h5>5.4.Conditional Statements and Loops:</h5>
<h5>Condition statement is a very simple concept - but it is how computer software makes "choices". Loops
and iteration complete our four basic programming patterns. Loops are the way we tell Python to do
something over and over. Loops are the way we build programs that stay with a problem until the
problem is solved</h5>
<h5>5.5.Functions and Array:</h5>
<h5>This is the lecture to learn about the implementations of the concepts we studied above with the help of
functions and arrays.</h5>
<h5>5.6.Classes/Objects, Math, JSON:</h5>
<h5>In this lecture, we will create classes/object to learn the object oriented programming. We work with
Application Program Interfaces / Web Services using the JavaScript Object Notation (JSON) data format.</h5>
<h5>5.7.File Handling:</h5>
<h5>In this lecture, we will train the students to handle Excel, CSV and JSON files. You can read and write
these files to play with the datasets.</h5>
<h5>5.8.Data Analysis:</h5>
<h5>The lecture will involve all the elements of the specialization. In the first part of the capstone, students
will do some visualizations to become familiar with the technologies in use and then will pursue their
own project to visualize some other data that they have or can find</h5>
<h5>5.9.Introduction to Big Data and PySpark (part 1):</h5>
<h5>Big Data as the digital trace that we are generating in this digital era. In this course, you will learn about
the characteristics of Big Data and its application in Big Data Analytics. You will gain an understanding
about the features, benefits, limitations, and applications of some of the Big Data processing tools. And
basic into of the PySpark.</h5>
<h5>5.10. Introduction to Big Data and PySpark (part 2):</h5>
<h5>Apache Spark is an open-source processing engine that provides users new ways to store and make use of
big data. It is an open-source processing engine built around speed, ease of use, and analytics. In this
course, you will discover how to leverage Spark to deliver reliable insights. The course provides an
overview of the platform, going into the different components that make up Apache Spark.</h5>
<h5>5.11. Database:</h5>
<h5>This lecture will introduce students to the basics of the Structured Query Language (SQL) as well as basic
database design for storing data as part of a multi-step data gathering, analysis, and processing effort</h5>
<h5>5.12. MongoDB:</h5>
<h5>This lecture will cover the emerging database technology i.e. NoSQL/MongoDB design for storing data
as part of a multi-step data gathering, analysis, and processing effort</h5>
<h5>5.13. Integration:</h5>
<h5>This lecture will cover the integration of the Apache Spark with the databases of MogoDB and MySQL to
perform ETL operations to make data pipline to manage streaming data like stocks data.</h5>
<h5>5.14. Pipeline and ETL (part 1):</h5>
<h5>After integrating the Databases, we need to perform operations on databases using apache spark. This
lecture will include the basic operations and concepts of ETL</h5>
<h5>5.15. Pipeline and ETL (part 2):</h5>
<h5>This lecture will include the advanced operations of the databases with apache spark and perform high
level of ETL methods to deal with streaming data.</h5>
<h5>5.16. Pipeline and ETL (part 3):</h5>
<h5>We use Cloud to make the data processing faster and secure. This lecture will be covering integration of
the apache spark with cloud.</h5>
<h5>5.17. Hadoop and Hive SQL (Part 1):</h5>
<h5>Hadoop is an open-source framework that allows for the distributed processing of large data sets across
clusters of computers using simple programming models. Hive, a data warehouse software, provides an
SQL-like interface to efficiently query and manipulate large data sets residing in various databases and
file systems that integrate with Hadoop.</h5>
<h5>5.18. Hadoop and Hive SQL (Part 2):</h5>
<h5>Hadoop concepts of MapReduce and HIVE will be covered in this lectures. This lecture will train the
individuals to deal with implementation of big data concepts and streaming data.</h5>
